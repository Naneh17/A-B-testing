# AB-testing
This project explores and compares two widely used Multi-Armed Bandit (MAB) algorithms:

Epsilon-Greedy with Exponential Decay – Balances exploration and exploitation by gradually reducing the exploration rate over time.

Thompson Sampling using the Beta Distribution – Utilizes Bayesian inference to make probabilistic decisions based on prior knowledge.

The objective is to evaluate how effectively these algorithms navigate the trade-off between exploring new options and exploiting known rewards to maximize cumulative rewards while minimizing regret.
